# Machine Learning Fundamentals for Programmers

This course will teach participants the fundamental building blocks of statistical learning theory. We will not focus on libraries or languages, assuming attendants to be capable of learning those on their own, but will instead focus on the theory needed to see opportunities and pick appropriate and implementable techniques to use with real-life data. Days will consist of lectures in the morning followed by open-ended exercises in the afternoons. Attendees are encouraged to bring data, ideas, products, and problems to work on specifically and will start on a personal project on the last day that can be continued after the course, professionally or personally. 


## Statistical Learning Theory

#### Topics: 
* History of Statistical Learning 
* Formulating the Problem: Approximation vs Estimation Error 
* Controlling the Bayes Risk 
* Overfitting 
* Bias / Variance 

#### Exercises: 
* Perceptron 
* K-nearest Neighbors 


## Regression and Continuous Distributions

#### Topics: 
* Expectations - Robustness, assumptions
* Entropy, Divergences and distance measures
* Gaussianity
* Bayesian Formulations
* Regularization

#### Exercises: 
* Condition Numbers and Inversions
* BYO Regularization
* Robust Regression
* Gaussian Processes


## Small Shapes in Big Data 

#### Topics:
* A Brief History of Data Visualization 
* Manifolds, Factors, and Lower-Dimensional Structure 
* Eigen Everywhere

#### Exercises: 
* Probablistic PCA
* Multidimensional Scaling


## Simple NLP

#### Topics: 
* Tokenization/BOW/TF-IDF
* Thinking of words as distributions
* Bayesian Hierarchical Modeling 
* Naive Bayes + LDA
* Embedding

#### Exercises: 
* Personal NLP Project. 


## Implementation
* Whirlwind Intro to Network/Graph Analysis 
* Optimization 
* Distributed Computing 
* Probablistic Programming 
* Languages!

#### Exercises:
* Final Personal Project. 

