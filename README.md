# Machine Learning Fundamentals for Programmers

A course for programmers to understand how algorithms learn from data and how they can be taken advantage of today.

### Outcomes
Participants will walk away with the ability to see basic opportunities, pick appropriate algorithms, and implement techniques that can be shipped in products today. They will be familiar with contemporary popular algorithms and have developed the theoretical foundations needed for deeper self-study within the field.

### Audience
This course is for those who have little or no background in machine learning or statistics, but a love for hacking, a background in programming, and basic working knowledge of at least one language with statistical libraries (Python, R, Java/Clojure/Scala, Matlab, etc.).

### Structure
Days will consist of morning lectures focused on building theoretical knowledge, followed by open-ended exercises in the afternoon, completed in any language/framework of the students' preference, designed to demystify the theory and show the underlying hackability of these ideas.

Everyone is encouraged to bring data, ideas, products, and problems to work on specifically and will start on a personal project on the last day that can be continued after the course, professionally or personally.


## Precourse
* Intro to Probability/Statistics

## Day 0

The Sunday before class starts we will have a short afternoon session to go over probability basics. This will be followed by beers.

## Day 1: Kaggle Competitions

The first day is for getting your hands dirty and implementing real prediction algorithms with the latest libraries and tools, leaving theory in the dust and gaining familiarity through usage. We introduce a series of modern tools and everyone will compete in a data hackathon that will last into the evening.

#### Topics:
* A Practical Introduction to Errors
* Languages and Libraries for Machine Learning
* Modern Winning Algorithms

#### Exercises:
* Data Hackathon: Kaggle Competitions.

## Day 2: Statistical Learning Theory

Here we go all the way back to the beginning of the field and formulate the problem formally: what does it mean to model the world probablistically. We then look at two of the most fundamental ideas that will be woven throughout the week: what does it mean for a model to overfit and how can we trade off the bias and variance of a model. Exercises consist of exploring and proving these ideas in code.

#### Topics:
* History of Statistical Learning
* Formulating the Problem: Approximation vs Estimation Error
* Controlling the Bayes Risk
* Overfitting
* Bias / Variance

#### Exercises:
* Perceptron
* K-nearest Neighbors


## Day 3: Regression and Continuous Distributions

We push ideas of probabilistic thinking further by considering how different assumptions about the randomness in the world can lead to very different conclusions. We introduce Bayesian statistics to show how formalizing prior knowledge allows us to learn more from the data than we would otherwise be able to learn.

#### Topics:
* What does the mean really tell you?
* Probability distributions and noise.
* Bayes' Rule. Bayesian Regression.
* Latent variables. Underfitting!
* Regularization

#### Exercises:
* Condition Numbers and Inversions
* BYO Regularization
* Robust Regression
* Gaussian Processes


## Day 4: Small Shapes in Big Data

Data often consists of many variables and in order to generalize we believe the same outcomes can be described with much fewer, even if we need to create them ourselves. Here we understand what it means to work with high-dimensional data, and solidify how we think about dimensionality.

#### Topics:
* A Brief History of Data Visualization
* Manifolds, Factors, and Lower-Dimensional Structure
* Sparsity
* The Curse of Dimensionality
* Egeindecomposition & PCA

#### Exercises:
* Probablistic PCA
* Multidimensional Scaling


## Day 5: Simple NLP & Discrete Distributions

We will explore several simple techniques in Natural Language Processing (NLP) that are extremely effective in practice and also because it will allow us to extend our knowledge of Bayesian formulations and see again how reasonable assumptions allow us to learn vast amounts from our data. We'll briefly introduce some other ideas, and students will work on their final personal project for the rest of the day.

#### Topics:
* Preprocessing, Tokenization, Bag-Of-Words Models, TF-IDF.
* Discrete distributions.
* Word and Document Embedding
* Intro to Network/Graph Analysis
* Optimization
* Probablistic Programming

#### Exercises:
* Final Personal Project.


## Day 5+: Workday

Saturday will be an optional day to come in and continue working on personal projects, ask questions, etc.
